# Med-VQA: Medical Visual Question Answering

## Project Overview
This project compares a discriminative baseline (**ResNet+BERT**) against a generative approach (**BLIP**) for answering questions about medical images (VQA-RAD dataset).

## Objectives
1. **Compare Architectures:** CNN-BERT vs. Vision Transformers.
2. **Evaluate Reasoning:** Analyze performance on Closed (Yes/No) vs. Open-ended questions.

## Dataset
We use the **VQA-RAD** dataset, containing 315 radiology images and 3,515 QA pairs.

## How to Run
The preliminary analysis can be found in `Preliminary_Data_Analysis.ipynb`.
